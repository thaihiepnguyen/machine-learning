{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc607505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "979c5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    \"\"\"\n",
    "    Support Vector Machine with SMO\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=1000, kernel_type=\"linear\", C=1, epsilon=0.0001):\n",
    "        self.kernels = {\n",
    "            \"linear\": self.kernel_linear,\n",
    "            \"quadratic\": self.kernel_quadratic,\n",
    "            \"rbf\": self.kernel_rbf\n",
    "        }\n",
    "        self.kernel = self.kernels[kernel_type]\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "        self.W = None # weights\n",
    "        self.b = None # bias\n",
    "        \n",
    "    def kernel_linear(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Linear kernel: k(x1, x2) = x1*x2.T\n",
    "        \"\"\"\n",
    "        return np.dot(x1, x2.T)\n",
    "    \n",
    "    def kernel_quadratic(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Quadratic kernel: k(x1, x2) = (x1*x2.T)^2\n",
    "        \"\"\"\n",
    "        return self.kernel_linear(x1, x2) ** 2\n",
    "    \n",
    "    def kernel_rbf(self, x1, x2):\n",
    "        \"\"\"\n",
    "        RBF kernel: k(x,z) = e^((x-z)^2/o^2)\n",
    "        \"\"\"\n",
    "        return math.exp((np.linalg.norm(x1-x2)**2)/ -0.1**2)\n",
    "    \n",
    "    def get_random_int(self, a, b, z):\n",
    "        \"\"\"\n",
    "        return random int in the range of [a, b] excluding z.\n",
    "        \"\"\"\n",
    "        r = list(range(a,z)) + list(range(z+1, b))\n",
    "        return random.choice(r)\n",
    "    \n",
    "    def calc_w(self, X, y, alpha):\n",
    "        \"\"\"\n",
    "        W = a*y*X\n",
    "        \"\"\"\n",
    "        return np.dot(X.T, np.multiply(alpha, y))\n",
    "    \n",
    "    def calc_b(self, X, y, W):\n",
    "        \"\"\"\n",
    "        b = 1/n * (y - X*W)\n",
    "        \"\"\"\n",
    "        return np.mean(y - np.dot(X, W))\n",
    "    \n",
    "    def E(self, X_k, y_k, W, b):\n",
    "        \"\"\"\n",
    "        calculate prediction error.\n",
    "        err = sign(Xk*W + b) - yk\n",
    "        \"\"\"\n",
    "        return np.sign(np.dot(X_k, W) + b) - y_k\n",
    "        \n",
    "    def compute_L_H(self, X, alpha_prime_j, alpha_prime_i, y_j, y_i):\n",
    "        \"\"\"\n",
    "        calculate bounds for aj\n",
    "        \"\"\"\n",
    "        if y_i != y_j:\n",
    "            return max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j)\n",
    "        else:\n",
    "            return max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        alpha = np.zeros((n))\n",
    "        \n",
    "        for itr in range(self.max_iter):\n",
    "            \n",
    "            alpha_prev = np.copy(alpha)\n",
    "            \n",
    "            for j in range(0, n):\n",
    "                # get random int i, where i!=j\n",
    "                i = self.get_random_int(0, n-1, j)\n",
    "                \n",
    "                # calcuating the second dericative of the objective function along the diagonal line:\n",
    "                kij = self.kernel(X[i], X[i] + self.kernel(X[j], X[j]) - 2*self.kernel(X[i], X[j]))\n",
    "        \n",
    "                if kij <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # storing the current values of ai and aj\n",
    "                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n",
    "                \n",
    "                # calculate bounds for aj\n",
    "                L, H = self.compute_L_H(self.C, alpha_prime_j, alpha_prime_i, y[i], y[i])\n",
    "        \n",
    "                # compute model parameters\n",
    "            \n",
    "                self.W = self.calc_w(X, y, alpha)\n",
    "                self.b = self.calc_b(X, y, self.W)\n",
    "                \n",
    "                # compute Ei, Ej\n",
    "                E_i = self.E(X[i], y[i], self.W, self.b)\n",
    "                E_j = self.E(X[j], y[j], self.W, self.b)\n",
    "                \n",
    "                # ........................\n",
    "                # set new alpha values:\n",
    "                # ........................\n",
    "                # aj new = aj + yj(Ei-Ej)/kij\n",
    "                alpha[j] = alpha_prime_j + float(y[i] * (E_i - E_j)) / kij\n",
    "                \n",
    "                # aj clipped:\n",
    "                alpha[j] = max(alpha[j], L)\n",
    "                alpha[j] = min(alpha[j], H)\n",
    "                \n",
    "                # ai:\n",
    "                alpha[i] = alpha_prime_i + y[i]*y[i]*(alpha_prime_j - alpha[j])\n",
    "                \n",
    "            # check convergence\n",
    "            # check if there is a difference between previous alpha and the current, if the change is smaller\n",
    "            # than epsilon, we stop, cause we found our support vectors. otherwise keep going untill convergence\n",
    "            diff = np.linalg.norm(alpha - alpha_prev)\n",
    "            if diff < self.epsilon:\n",
    "                break\n",
    "        \n",
    "        # compute final model parameters\n",
    "        self.W = self.calc_w(X, y, alpha)\n",
    "        self.b = self.calc_b(X, y, self.W)\n",
    "        \n",
    "        # get support vectors\n",
    "        idx = np.where(alpha>0)[0]\n",
    "        SV = X[idx]\n",
    "        \n",
    "        return SV\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.W) + self.b)\n",
    "    \n",
    "    def get_accuracy(self, X, y):\n",
    "        correct = self.predict(X) == y\n",
    "        acc = sum(correct) / len(y) * 100\n",
    "        return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd347b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = \"diabetes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# change labels to be {1, -1}\n",
    "df = df.values\n",
    "X = df[:,:-1]\n",
    "y = df[:,-1]\n",
    "y = 2*y-1\n",
    "\n",
    "# split data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "\n",
    "# train the model\n",
    "\n",
    "svm_model = SVMClassifier(kernel_type=\"rbf\")\n",
    "sv = svm_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aa5b050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.28571428571429"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.get_accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca89f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
